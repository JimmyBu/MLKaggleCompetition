# -*- coding: utf-8 -*-
"""Copy of ASS4Q8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M6sqolZoZaLV4L0AuUKjjeor0a9GV57D

EXPLORATION OF THE DATA
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, f1_score
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Dropout, Input, Conv1D, MaxPooling1D, GlobalAveragePooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler
from tensorflow.keras.utils import plot_model, to_categorical
from tensorflow.keras import regularizers
import torch
from torch.utils.data import TensorDataset, DataLoader

from google.colab import drive
drive.mount('/content/drive')
train_path = '/content/drive/My Drive/train.csv'
test_path = '/content/drive/My Drive/test.csv'

# Load the dataset
df_train = pd.read_csv(train_path)
df_test = pd.read_csv(test_path)

"""DATA PREPROCESSING"""

# Preprocessing
scaler = StandardScaler()
X_train = df_train.drop(columns=['ID', 'Label'])
y_train = df_train['Label']
X_test = df_test.drop(columns=['ID'])

X_train_np = X_train.values
y_train_np = y_train.values
X_test_np = X_test.values

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled = X_train_scaled.reshape(-1, 300, 100) # here
X_test_scaled = X_test_scaled.reshape(-1, 300, 100) # here

# # Splitting data for training and validation
# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
# Y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)

X_train_split, X_val, y_train_split, y_val = train_test_split(X_train_scaled, y_train_np, test_size=0.2, random_state=42)

# train_dataset = TensorDataset(X_train_split, y_train_split)
# val_dataset = TensorDataset(X_val, y_val)

# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)
# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)

# One-hot encoding for target variables

# One-hot encode the labels
num_classes = len(np.unique(y_train_np))
y_train_one_hot = to_categorical(y_train_np, num_classes=num_classes)
y_val_one_hot = to_categorical(y_val, num_classes=num_classes)

# Check dimensions to ensure compatibility
print(f"X_train_scaled shape: {X_train_scaled.shape}")
print(f"y_train_one_hot shape: {y_train_one_hot.shape}")
print(f"X_val_scaled shape: {X_val.shape}")
print(f"y_val_one_hot shape: {y_val_one_hot.shape}")

# Convert numpy arrays to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train_one_hot))
val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val_one_hot))

# Shuffle, batch, and prefetch the training dataset
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.AUTOTUNE)
val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)

LATENT_SIZE = 32
input_size = X_train_scaled.shape[1]

from tensorflow.keras.layers import LeakyReLU

def network(X_train, y_train, X_val, y_val):
    # Define the input shape
    im_shape = (300, 100)

    inputs = Input(shape=im_shape)

    # Input layer
    x = Conv1D(128, kernel_size=3)(inputs)
    x = LeakyReLU(alpha=0.01)(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Conv1D(64, kernel_size=3)(x)
    x = LeakyReLU(alpha=0.01)(x)
    x = GlobalAveragePooling1D()(x)
    x = Dense(128, kernel_regularizer=regularizers.l2(0.01))(x)
    x = LeakyReLU(alpha=0.01)(x)
    x = Dropout(0.5)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    # Compile the model
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Learning rate scheduler function
    def scheduler(epoch, lr):
        return float(lr * tf.math.exp(-0.1)) if epoch > 10 else float(lr)

    # Callbacks
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),
        ModelCheckpoint(filepath='best_model.keras', monitor='val_loss', save_best_only=True),
        LearningRateScheduler(scheduler),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
    ]

    # Train the model
    history = model.fit(
        X_train, y_train,
        epochs=100,
        callbacks=callbacks,
        batch_size=32,
        validation_data=(X_val, y_val)
    )

    # Load best model weights
    model.load_weights('best_model.keras')

    return model, history

num_classes = len(np.unique(y_train))

model, history = network(X_train_scaled, y_train_one_hot, X_val, y_val_one_hot)

# Predict on the test set
X_test_reshaped = X_test_scaled
y_test_pred_prob = model.predict(X_test_reshaped)

# Convert probabilities to class labels
y_test_pred_labels = np.argmax(y_test_pred_prob, axis=1)

# Prepare results DataFrame
results_df = pd.DataFrame({
    'ID': df_test['ID'],
    'Label': y_test_pred_labels
})

# Save results to CSV
results_df.to_csv('submission.csv', index=False)
print('submission.csv')

def plot_training_history(history):
    # Extract loss and accuracy from the history object
    history_dict = history.history
    epochs = range(1, len(history_dict['loss']) + 1)

    # Plot training and validation loss
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history_dict['loss'], 'b', label='Training loss')
    plt.plot(epochs, history_dict['val_loss'], 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Plot training and validation accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history_dict['accuracy'], 'b', label='Training accuracy')
    plt.plot(epochs, history_dict['val_accuracy'], 'r', label='Validation accuracy')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Assuming `history` is returned from the `network` function
plot_training_history(history)